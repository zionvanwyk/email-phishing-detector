{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zionvanwyk/email-phishing-detector/blob/main/Additional_Features_Phishing_Email_Model_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vyHIAt8RcdfG"
      },
      "source": [
        "# Set up environment\n",
        "\n",
        "*   Install dependencies.\n",
        "*   Connect to Huggingface to access the uploaded dataset via access token.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "abOTOTKTb3hT",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!pip install transformers evaluate accelerate pandas scikit-learn\n",
        "!pip install huggingface_hub\n",
        "!pip install torch\n",
        "!pip install -U datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "notebook_login()\n"
      ],
      "metadata": {
        "id": "VtncglGNBQBj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Upload dataset\n",
        "\n",
        "*   Used phishing_email.csv from the Kaggle phishing email dataset that can be found here: https://www.kaggle.com/datasets/naserabdullahalam/phishing-email-dataset\n",
        "\n",
        "\n",
        "*   This is a combination of six datasets. The \"text_combined\" column is the central element of the final dataset in this phishing email analysis. It combines the subject line, the body, date, and sender email text of the emails from the initial datasets.\n"
      ],
      "metadata": {
        "id": "R17sRMXaBF8g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"zionia/phishing-emails\")"
      ],
      "metadata": {
        "id": "RS4xLd_dy0nc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenize the Dataset\n",
        "\n",
        "*   Tokenize data using AutoTokenizer.\n",
        "*   Add custom features:\n",
        "        * text_lengths: Total character count of each email text.\n",
        "        * token_counts: Number of whitespace-separated tokens (words/symbols) in each email.\n",
        "        * avg_token_lengths: Average length (characters per token) for each email.\n",
        "        * num_date_tokens: Count of date-related tokens (months, years) in email.\n",
        "        * has_attachment_references: Binary flag (1/0) indicating if the email mentions file attachments (e.g., .pdf, \"attached\").\n",
        "        * has_operational_keywords: Binary flag (1/0) for operational terms (e.g., \"report\", \"schedule\", \"data\").\n",
        "        * has_phishy_keywords: Binary flag (1/0) for suspicious phrases (e.g., \"urgent\", \"login\", \"verify\").\n",
        "*   Format the dataset for the trainer to use the custom features."
      ],
      "metadata": {
        "id": "-atlFtBYB2PX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "import numpy as np\n",
        "import evaluate\n",
        "import os\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    tokenized_output = tokenizer(\n",
        "        examples[\"text_combined\"],\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=256\n",
        "    )\n",
        "\n",
        "    text_lengths = []\n",
        "    token_counts = []\n",
        "    avg_token_lengths = []\n",
        "    num_date_tokens = []\n",
        "    has_attachment_references = []\n",
        "    has_operational_keywords = []\n",
        "    has_phishy_keywords = []\n",
        "\n",
        "    for text in examples[\"text_combined\"]:\n",
        "        text_length = len(text)\n",
        "\n",
        "        tokens = text.lower().split()\n",
        "        token_count = len(tokens)\n",
        "        avg_token_length = sum(len(token) for token in tokens) / token_count if token_count > 0 else 0\n",
        "\n",
        "        date_related = {\"jan\", \"feb\", \"mar\", \"apr\", \"may\", \"jun\", \"jul\", \"aug\", \"sep\", \"oct\", \"nov\", \"dec\",\n",
        "                       \"january\", \"february\", \"march\", \"april\", \"may\", \"june\", \"july\", \"august\",\n",
        "                       \"september\", \"october\", \"november\", \"december\",\n",
        "                       \"2001\", \"2002\", \"2003\", \"2004\", \"2005\", \"2006\", \"2007\", \"2008\", \"2009\", \"2010\",\n",
        "                       \"2011\", \"2012\", \"2013\", \"2014\", \"2015\", \"2016\", \"2017\", \"2018\", \"2019\", \"2020\",\n",
        "                       \"2021\", \"2022\", \"2023\", \"2024\", \"2025\"}\n",
        "        num_date_token = sum(1 for token in tokens if token in date_related)\n",
        "\n",
        "        attachment_ref = any(ext in text.lower() for ext in [\".xls\", \".xlsx\", \".pdf\", \".doc\", \".docx\",\n",
        "                                                           \"attachment\", \"attached\", \"file\"])\n",
        "        operational_keywords = any(word in text.lower() for word in [\"nom\", \"actual\", \"vols\", \"schedule\",\n",
        "                                                                   \"attached\", \"report\", \"data\", \"summary\"])\n",
        "        phishy_keywords = any(word in text.lower() for word in [\"verify\", \"urgent\", \"login\", \"click\",\n",
        "                                                               \"bank\", \"account\", \"update\", \"password\",\n",
        "                                                               \"security\", \"alert\", \"confirm\", \"immediately\"])\n",
        "        text_lengths.append(text_length)\n",
        "        token_counts.append(token_count)\n",
        "        avg_token_lengths.append(avg_token_length)\n",
        "        num_date_tokens.append(num_date_token)\n",
        "        has_attachment_references.append(int(attachment_ref))\n",
        "        has_operational_keywords.append(int(operational_keywords))\n",
        "        has_phishy_keywords.append(int(phishy_keywords))\n",
        "\n",
        "    tokenized_output[\"text_length\"] = text_lengths\n",
        "    tokenized_output[\"token_count\"] = token_counts\n",
        "    tokenized_output[\"avg_token_length\"] = avg_token_lengths\n",
        "    tokenized_output[\"num_date_tokens\"] = num_date_tokens\n",
        "    tokenized_output[\"has_attachment_reference\"] = has_attachment_references\n",
        "    tokenized_output[\"has_operational_keywords\"] = has_operational_keywords\n",
        "    tokenized_output[\"has_phishy_keywords\"] = has_phishy_keywords\n",
        "\n",
        "    return tokenized_output\n",
        "\n",
        "tokenized_dataset = dataset.map(tokenize_function, batched=True)"
      ],
      "metadata": {
        "id": "MOXMqfH9rUOr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def format_dataset_for_trainer(dataset):\n",
        "    def to_tensor(example):\n",
        "        engineered_features = torch.tensor([\n",
        "            example[\"text_length\"],\n",
        "            example[\"token_count\"],\n",
        "            example[\"avg_token_length\"],\n",
        "            example[\"num_date_tokens\"],\n",
        "            example[\"has_attachment_reference\"],\n",
        "            example[\"has_operational_keywords\"],\n",
        "            example[\"has_phishy_keywords\"]\n",
        "        ], dtype=torch.float)\n",
        "\n",
        "        return {\n",
        "            'input_ids': torch.tensor(example['input_ids']),\n",
        "            'attention_mask': torch.tensor(example['attention_mask']),\n",
        "            'labels': torch.tensor(example['label']),\n",
        "            'extra_features': engineered_features\n",
        "        }\n",
        "\n",
        "    return dataset.map(to_tensor)\n"
      ],
      "metadata": {
        "id": "Tg0tt9NVsEEx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "formatted_dataset = {\n",
        "    \"train\": format_dataset_for_trainer(tokenized_dataset[\"train\"]),\n",
        "    \"test\": format_dataset_for_trainer(tokenized_dataset[\"test\"]),\n",
        "}"
      ],
      "metadata": {
        "id": "cJYI7BoOwoZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train the Model\n",
        "\n",
        "*  Chosen model is DistilBERT.\n",
        "*  Add the 7 custom features to the configuration for fine-tuning.\n",
        "*  Train model and push to Huggingface hub."
      ],
      "metadata": {
        "id": "AI8C_hjYECAO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DistilBertPreTrainedModel, DistilBertModel\n",
        "\n",
        "class DistilBertWithFeatures(DistilBertPreTrainedModel):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.num_labels = config.num_labels\n",
        "        self.distilbert = DistilBertModel(config)\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "        self.classifier = nn.Linear(768 + 7, config.num_labels)\n",
        "        self.init_weights()\n",
        "\n",
        "    def forward(self, input_ids=None, attention_mask=None, extra_features=None, labels=None, **kwargs):\n",
        "        distilbert_output = self.distilbert(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask\n",
        "        )\n",
        "        hidden_state = distilbert_output[0]\n",
        "        pooled_output = hidden_state[:, 0]\n",
        "\n",
        "        if extra_features is not None:\n",
        "            pooled_output = torch.cat((pooled_output, extra_features), dim=1)\n",
        "\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        logits = self.classifier(pooled_output)\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss_fct = nn.CrossEntropyLoss()\n",
        "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
        "\n",
        "        return {'loss': loss, 'logits': logits} if loss is not None else logits"
      ],
      "metadata": {
        "id": "EnS5SMOwxC7G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DistilBertConfig\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "config = DistilBertConfig.from_pretrained(\n",
        "    \"distilbert-base-uncased\",\n",
        "    num_labels=2,\n",
        "    dropout=0.1\n",
        ")\n",
        "model = DistilBertWithFeatures.from_pretrained(\n",
        "    \"distilbert-base-uncased\",\n",
        "    config=config\n",
        ")"
      ],
      "metadata": {
        "id": "X5bQtHkvy9rw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from transformers import Trainer, TrainingArguments\n",
        "from datasets import Dataset\n",
        "import torch\n",
        "import numpy as np\n",
        "import evaluate\n",
        "\n",
        "class CustomTrainer(Trainer):\n",
        "    def compute_loss(self, model, inputs, return_outputs=False):\n",
        "        labels = inputs.pop(\"labels\")\n",
        "        extra_features = inputs.pop(\"extra_features\")\n",
        "        outputs = model(**inputs, extra_features=extra_features)\n",
        "        loss = torch.nn.functional.cross_entropy(outputs, labels)\n",
        "        return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    push_to_hub=True,\n",
        "    hub_model_id=\"zionia/email-phishing-detector\",\n",
        "    hub_strategy=\"end\",\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    accuracy = evaluate.load(\"accuracy\").compute(predictions=predictions, references=labels)['accuracy']\n",
        "    precision = evaluate.load(\"precision\").compute(predictions=predictions, references=labels, average='binary')['precision']\n",
        "    recall = evaluate.load(\"recall\").compute(predictions=predictions, references=labels, average='binary')['recall']\n",
        "    f1 = evaluate.load(\"f1\").compute(predictions=predictions, references=labels, average='binary')['f1']\n",
        "    return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=formatted_dataset[\"train\"],\n",
        "    eval_dataset=formatted_dataset[\"test\"],\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ],
      "metadata": {
        "id": "SVCk00dPLg8N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "trainer.train()\n",
        "trainer.push_to_hub(\"Training complete - basic model (with feature extraction)\")\n",
        "tokenizer.push_to_hub(\"zionia/email-phishing-detector\")"
      ],
      "metadata": {
        "id": "b3Xy8SVXLwQH"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}